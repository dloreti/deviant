% !TEX root = ../deviant.tex

\section{Motivations}
\label{sec:motivations}
Process discovery focuses on the analysis of an event log in order to automatically learn the process model underpinning the cases of such input log. 
In general, the techniques in this field assume that the input log is not the complete elicitation of all the expected cases. Other traces not reported in there might be deemed compliant with the expected behaviour of the system. Therefore, the aim of process discovery techniques is necessarily to generalise the log by finding a compact way to express the usual behaviour of the systems, for example, by means of a structured or declarative process model. 
Such generalisation causes the resulting model to allow as compliant a larger set of traces w.r.t. the input log \cite{2011-Aalst}. This is one of the challenges of process discovery. If we could rely on logs reporting all expected behaviours, the extraction of the model would be a rather straightforward task, because we would need to learn a model exhaustively covering all the elicited cases. Also, if the model going to be learned is aimed for a following compliance checking task, model extraction would not be crucial, because a simple algorithm verifying the membership of a trace to the input set would serve the purpose.
On the other hand, in order to avoid the so-called ``spaghetti'' models, process discovery must also prevent overfitting \cite{2010-Aalst}. To this end, a widespread strategy is to overlook those process cases that show a particularly infrequent behaviour. A practice that is often obtained by checking that the extracted model constraints meet certain thresholds according to predefined metrics (e.g. fitness, precision, generality, and simplicity). 
It is therefore evident that, besides the usual attempt to generalise while extracting the model, process discovery necessarily performs an opposite attempt to increase the specificity by excluding some traces from the learning task.

The overlooked traces represent a sort of ``stranger'' behaviour, a deviation from the usual and expected conduct, which is the subject of \emph{deviance mining}\cite{2016-Nguyen}. 
%
Indeed, deviance mining is a field of process mining that encompasses techniques precisely to explain the reasons why a business process deviates from its normal execution. 
According to theory, deviations can have a positive or negative connotation. Positive deviations refer to desirable cases, where the business process shows particularly high performance, such as short execution time, low  cost, or particularly profitable outcomes \cite{2004-Spreitzer}. On the contrary, negative deviances usually refer to unwanted cases, that is situations non-conforming with the expected behaviour---because, for example, they produce an unwanted outcome or exceed the conventional execution time or cost \cite{2016-Nguyen}.
 
Most process discovery techniques do not consider negative examples. Indeed, a widespread position in this regard is that negative traces are usually not available. Event logs usually report a restricted number of non-compliant or unwanted case blurred in a much larger number of positive and more common examples. Nonetheless, we could say that even those process discovery techniques that are not explicitly based on the availability of negative example, actually make use of them in an implicit way, by assuming that they are somewhere present in the log and stating that they must be overlooked.
Differently from most previous approaches, we believe that negative (as well as positive) deviances might still have some informative content that is important to consider when discovering the process model. A more conscious shift is needed toward considering not only the positive and usual traces in the log, but also all the others, which provide information on negative and/or less frequent cases.

Another popular practice among process discovery techniques is that of defining a language bias, that clarifies the order in which the template of constraints must be considered during the learning task. Indeed, given a certain language to express the business model, different results may arouse depending from which patterns we search first among the log's cases. The choice of a language bias over another can be seen as a way to drive the discovery process towards a certain direction. Obviously, different results correspond to different ways to classify those traces that were not yet observed i.e., those case that are not present in the log, but might occur in the future.
In a sense, the language bias required by some process discovery techniques is a sort of implicit heuristic to decide in which order we must perform the generalisation or specialisation steps, while composing the business model.

Besides the benefit of taking into account deviant traces, we claim that process discovery should be able to take advantage from a more explicit heuristic to explore the search space. Among all possible behavioural patterns occurring in the logs, the choice of one over another should be driven by a more explicable strategy than simply defining an order of preferred constraints to be considered.
 
%\tcolor{blue}{On one hand, it is important to provide an easy way to label the traces in the log, so that positive and negative, usual and unusual cases are semantically separated and the information they carry can be exploited to refine the process model}\tododl{reword cause we do not focus on labelling. We need it.}. If negative examples are not present in the log, it is nonetheless important to clearly identify which traces are currently excluded by the discovered model because the characteristics of such excluded traces might still have significance for the model itself.
%On the other hand, the discovery process must operate taking into account all the traces in the log, extracting information from both common and less frequent traces, and providing a simple way to balance the discovered model between overfitting and underfitting. In order to achieve this, the discovery task will have to explore an extremely large space of possible models



